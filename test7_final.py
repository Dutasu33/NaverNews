#ì•„ë˜ì˜ íŒŒì¼ì€ ìŠ¤íŠ¸ë¦¼ë¦¿ ë° ì…€í¬ê¸°,ë‚ ì§œí•„ì²˜ í¬í•¨ëœ ì½”ë“œì´ë‹¤.

import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
import pandas as pd
import time
import datetime
import requests
import io
import urllib.parse

# âœ… í˜ì´ì§€ ë ˆì´ì•„ì›ƒ ì„¤ì •
st.set_page_config(layout="wide")
st.title('ğŸ“° Naver News Search')

# âœ… ì‚¬ìš©ì ì…ë ¥: ê²€ìƒ‰ì–´ + ë‚ ì§œ ë²”ìœ„
search_query = st.text_input('ğŸ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:')
today = datetime.date.today()
start_date = st.date_input("ğŸ“… ì‹œì‘ ë‚ ì§œ", today - datetime.timedelta(days=7))
end_date = st.date_input("ğŸ“… ì¢…ë£Œ ë‚ ì§œ", today)

# ê²€ìƒ‰ ë²„íŠ¼ ì¶”ê°€
search_button = st.button('ê²€ìƒ‰ ì‹œì‘')

# âœ… í…Œì´ë¸” CSS ìŠ¤íƒ€ì¼ ì •ì˜
st.markdown("""
<style>
table {
    table-layout: fixed;
    width: 100%;
    border-collapse: collapse;
}
th:nth-child(1), td:nth-child(1) {  /* ë²ˆí˜¸ */
    width: 5%;
    text-align: center;
    font-size: 0.9em;
}
th:nth-child(2), td:nth-child(2) {  /* ì œëª© */
    width: 30%;
}
th:nth-child(3), td:nth-child(3) {  /* ë‚´ìš© */
    width: 40%;
}
th:nth-child(4), td:nth-child(4) {  /* URL */
    width: 12%;
    font-size: 0.8em;
}
th, td {
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
    padding: 4px;
    border: 1px solid #ddd;
}
</style>
""", unsafe_allow_html=True)

# âœ… í¬ë¡¤ë§ ì‹¤í–‰ ì¡°ê±´
if search_button and search_query and start_date <= end_date:
    # ë‚ ì§œ í¬ë§· ë³€í™˜
    start_str = start_date.strftime('%Y.%m.%d')
    end_str = end_date.strftime('%Y.%m.%d')

    # í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •
    chrome_driver_path = r"D:\backup\Downloads\chromedriver-win64 (1)\chromedriver-win64\chromedriver.exe"
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    service = Service(chrome_driver_path)
    driver = webdriver.Chrome(service=service, options=options)

    # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL (ë‚ ì§œ í•„í„° í¬í•¨)
    encoded_query = urllib.parse.quote(search_query)
    url = f"https://search.naver.com/search.naver?ssc=tab.news.all&query={encoded_query}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={start_str}&de={end_str}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from{start_str.replace('.', '')}to{end_str.replace('.', '')}&is_sug_officeid=0&office_category=&service_area="

    # URL ì¶œë ¥
    st.write(f"Generated URL: {url}")

    driver.get(url)
    time.sleep(2)

    print(url)

    # ìŠ¤í¬ë¡¤ ë‹¤ìš´ìœ¼ë¡œ ë‰´ìŠ¤ ì¶”ê°€ ë¡œë”©
    for _ in range(3):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

    titles, contents, urls, image_urls = [], [], [], []
    news_boxes = driver.find_elements(By.CSS_SELECTOR, "div.sds-comps-vertical-layout")

    for box in news_boxes:
        try:
            title_elem = box.find_element(By.CSS_SELECTOR, "span.sds-comps-text-ellipsis-1.sds-comps-text-type-headline1")
            content_elem = box.find_element(By.CSS_SELECTOR, "span.sds-comps-text-ellipsis-3.sds-comps-text-type-body1")
            link_elem = box.find_element(By.CSS_SELECTOR, "a.bynlPWBHumGsbotLYK9A.jT1DuARpwIlNAFMacxlu")
            image_elem = box.find_element(By.CSS_SELECTOR, "img")

            title = title_elem.text.strip()
            content = content_elem.text.strip()
            link = link_elem.get_attribute("href").strip()
            image_url = image_elem.get_attribute("src").strip()

            # ì¶”ê°€ëœ ë¡œì§: ì¤‘ë³µ í™•ì¸ ë° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if (title, link) not in zip(titles, urls):
                titles.append(title)
                contents.append(content)
                urls.append(link)
                image_urls.append(image_url)

        except Exception:
            continue

    driver.quit()

    # âœ… ë°ì´í„°í”„ë ˆì„ êµ¬ì„±
    df = pd.DataFrame({
        "ë²ˆí˜¸": range(1, len(titles) + 1),
        "ì œëª©": titles,
        "ë‚´ìš©": contents,
        "URL": urls,
    })
    df = df.drop_duplicates(subset='ì œëª©', keep='first').reset_index(drop=True)

    # Limit content to two lines
    df['ë‚´ìš©'] = df['ë‚´ìš©'].apply(lambda x: '<br>'.join(x.split('\n')[:2]))

    # âœ… HTMLìš© ì»¬ëŸ¼ì€ ë”°ë¡œ ìƒì„±
    df['URL_HTML'] = df['URL'].apply(lambda x: f'<a href="{x}" target="_blank">ê¸°ì‚¬ ë³´ê¸°</a>')

    # âœ… Streamlit ì›¹ì—ì„œ HTML ë§í¬ í‘œì‹œ (URL ì»¬ëŸ¼ ì‚­ì œ, URL_HTML ì»¬ëŸ¼ ìœ ì§€)
    st.write(f"âœ… ì´ {len(df)}ê°œì˜ ë‰´ìŠ¤ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
    st.write(df.drop(columns=['URL']).to_html(escape=False, index=False), unsafe_allow_html=True)

    # âœ… Excelì—ëŠ” HYPERLINK í•¨ìˆ˜ë§Œ ë“¤ì–´ê°€ë„ë¡ ë”°ë¡œ ìƒì„±
    df['ê¸°ì‚¬ ë³´ê¸°'] = df['URL'].apply(lambda x: f'=HYPERLINK("{x}", "ê¸°ì‚¬ ë³´ê¸°")' if pd.notna(x) else "")

    # â— ì—‘ì…€ì—ëŠ” HTML íƒœê·¸ ë“¤ì–´ê°€ì§€ ì•Šê²Œ URL_HTML ì œê±°
    excel_df = df.drop(columns=['URL', 'URL_HTML'])

    # âœ… ì—‘ì…€ íŒŒì¼ ìƒì„±
    excel_buffer = io.BytesIO()
    excel_df.to_excel(excel_buffer, index=False)

    # âœ… CSV íŒŒì¼ ìƒì„±
    csv_buffer = io.StringIO()
    excel_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')

    # ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    start_str_filename = start_date.strftime('%Y%m%d')
    end_str_filename = end_date.strftime('%Y%m%d')

    # íŒŒì¼ ì´ë¦„ì— ê²€ìƒ‰ì–´ì™€ ë‚ ì§œ ë²”ìœ„ í¬í•¨
    file_name = f"ë„¤ì´ë²„ë‰´ìŠ¤_{search_query}_{start_str_filename}_{end_str_filename}.xlsx"

    # âœ… ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    st.download_button(
        label="â¬‡ï¸ ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œ",
        data=excel_buffer.getvalue(),
        file_name=file_name,
        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    )

    # âœ… CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    st.download_button(
        label="â¬‡ï¸ CSVë¡œ ë‹¤ìš´ë¡œë“œ",
        data=csv_buffer.getvalue(),
        file_name=file_name.replace('.xlsx', '.csv'),
        mime='text/csv'
    )

# â— ë‚ ì§œ ì˜¤ë¥˜ ì²˜ë¦¬
elif start_date > end_date:
    st.error("âŒ ì¢…ë£Œ ë‚ ì§œëŠ” ì‹œì‘ ë‚ ì§œë³´ë‹¤ ì´í›„ì—¬ì•¼ í•©ë‹ˆë‹¤.") 